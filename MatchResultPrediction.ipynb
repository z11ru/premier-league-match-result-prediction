{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch Predict: English Premier League Match Results Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "*Click to follow links.*\n",
    "\n",
    "Match Statistics: [matches.jsonl](https://www.kaggle.com/datasets/hugomathien/soccer/data) and [players.csv](https://www.kaggle.com/datasets/hugomathien/soccer/data)\n",
    "\n",
    "Player Statistics: [All Files in player_statistics folder](https://www.kaggle.com/datasets/davidantonioteixeira/premier-league-player-statistics-1992-2022?resource=download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Construction\n",
    "\n",
    "The datasets downloaded from the links above each contain part of, but not the entire, data required for training a machine learning model for our purposes. The `matches.jsonl` file contains match data, while the `players.csv` file contains information to map player names with player ids. The player statistics files contain player data, but separated across different seasons and for different statistics.\n",
    "\n",
    "The dataset construction process involves several key steps: merging player data, merging match data, mapping player names to IDs, and finally creating a unified dataset for ML training. Each step is encapsulated in a Python script, executed sequentially to ensure data integrity and alignment.\n",
    "\n",
    "**Merging Player Data `(merge_player_data.py)`**\n",
    "\n",
    "Individual CSV files containing player statistics are read from the ./player_statistics directory one at a time in a for loop.\n",
    "Columns not relevant to the analysis ('Rank', 'Club', 'Nationality') are removed.\n",
    "A unified DataFrame is created by merging individual DataFrames on 'Initial Year' and 'Player'. 'Initial Year' is the year the for which the player statistics are calculated - it represents the start of the Premier League season. However, since we want to predict match results based on player stats from the previous season, we need to align the player data with the match data accordingly. \n",
    "\n",
    "<span style = \"color:blue\"> This is achieved by adding 1 to all values in the initial year column. </span> This way, the player data for what was originally the 2020 season is now aligned with the match data for the 2021 season, and so on.\n",
    "\n",
    "<span style=\"color:blue\">Missing 'Minutes Played' values are filled with the column mean.</span>\n",
    "All statistics are normalized to per-minute-played metrics.\n",
    "\n",
    "**Merging Match Data `(merge_match_data.py)`**\n",
    "\n",
    "The script reads a JSONL file (matches.jsonl), dropping entries not pertaining to the English league (`league == en`). For each line in the input file, the script reads and parses the JSON object representing a match's statistics. It constructs a row for each match that includes the match ID, year, home and away goals, and identifiers for every player (starting and substitutes) from both home and away teams. These rows are written to the output CSV file, effectively transforming and merging the JSON Lines data into a structured CSV format to merge with player data above.\n",
    "\n",
    "**Map Players to Matches `(map_player_to_id.py)`**\n",
    "\n",
    "The players in player_data are identified using names, and for match data, they are identified using numerical IDs. This script maps player names to player IDs. The players.csv contains the names and IDs of players, but some are referred only by their nicknames. This script matches either full names to player IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('final_dataset.csv')\n",
    "X = data.drop(['MatchID', 'Result', 'Home Goal Difference'], axis=1)\n",
    "y = data['Result']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=11)\n",
    "\n",
    "# The Results column has 3 classes: -1, 0, 1. We will convert them to 0, 1, 2\n",
    "y_train = y_train.map({-1: 0, 0: 1, 1: 2})\n",
    "y_test = y_test.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully Connected Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1168439388275146\n",
      "Epoch 100, Loss: 0.7335348129272461\n",
      "Epoch 200, Loss: 0.13779860734939575\n",
      "Epoch 300, Loss: 0.013172315433621407\n",
      "Epoch 400, Loss: 0.004608034621924162\n",
      "Epoch 500, Loss: 0.0023675141856074333\n",
      "Epoch 600, Loss: 0.0014146986650303006\n",
      "Epoch 700, Loss: 0.0009380851406604052\n",
      "Epoch 800, Loss: 0.0006680914666503668\n",
      "Epoch 900, Loss: 0.0005013535264879465\n",
      "Epoch 1000, Loss: 0.0003896205744240433\n"
     ]
    }
   ],
   "source": [
    "class MatchPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MatchPredictor, self).__init__()\n",
    "        self.layer1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.out_layer = nn.Linear(32, 3)  # 3 classes for Home Win, Draw, Home Loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "model = MatchPredictor()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1000 + 1):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.84%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Instructs PyTorch not to calculate gradients\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    for inputs, labels in test_loader:  # Assuming you have a DataLoader for your test set\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 23 43]\n",
      " [30 25 41]\n",
      " [34 31 98]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test_tensor is your true labels for the test set and predictions is the list from above\n",
    "y_true = y_test_tensor.numpy()\n",
    "conf_matrix = confusion_matrix(y_true, predictions)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Home Loss       0.46      0.45      0.46       121\n",
      "        Draw       0.32      0.26      0.29        96\n",
      "    Home Win       0.54      0.60      0.57       163\n",
      "\n",
      "    accuracy                           0.47       380\n",
      "   macro avg       0.44      0.44      0.44       380\n",
      "weighted avg       0.46      0.47      0.46       380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true, predictions, target_names=['Home Loss', 'Draw', 'Home Win'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
